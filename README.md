# Improving Deep Neural Networks
Hyper Parameter Tuning and Batch Normalization, Gradient-Checking and Regularization
## Hyper Parameter tuning and Batch Normalization : 
Here are a few popular hyperparameters that are tuned for deep networks

- α (alpha): learning rate
- β (beta): momentum
- number of layers
- number of hidden units
- learning rate decay
- mini-batch size

There are others specific to optimisation techniques, for instance, you have β1, β2, and ε for the Adam optimisation.
PLease feel free to check the Hyper Parameter Tuning dicectory for detailed description
